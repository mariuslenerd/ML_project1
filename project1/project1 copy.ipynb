{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87e6083d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'cross_validation' from '/Users/christophersoriano/Library/Mobile Documents/com~apple~CloudDocs/Mac/Epfl/Master/MA3/Machine Learning/ML_project1/project1/cross_validation.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers import *\n",
    "import data_preprocessing\n",
    "import importlib\n",
    "importlib.reload(data_preprocessing)\n",
    "import cross_validation\n",
    "import TEST\n",
    "importlib.reload(TEST)\n",
    "importlib.reload(cross_validation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0e42811",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c76a2d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data\n",
    "x_train_raw, x_test_raw,y_train_raw, train_ids, test_ids = load_csv_data('dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9001d786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(328135, 321)\n",
      "(109379, 321)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_raw.shape)\n",
    "print(x_test_raw.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88380452",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reformat y_train to have 0's instead of -1's\n",
    "y_train_original = y_train_raw.copy()\n",
    "y_train_raw[y_train_raw == -1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f793521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109379, 321)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#replace NaN by a float to treat NaN as a categorical feature\n",
    "x_test_raw = np.nan_to_num(x_test_raw, nan = -10.0)\n",
    "x_test_raw.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75961310",
   "metadata": {},
   "source": [
    "### One Hot Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1db0f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_annoted = data_preprocessing.read_annotated_csv('dataset/data_anotated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cc22ef36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christophersoriano/Library/Mobile Documents/com~apple~CloudDocs/Mac/Epfl/Master/MA3/Machine Learning/ML_project1/project1/data_preprocessing.py:134: RuntimeWarning: invalid value encountered in divide\n",
      "  data = data / data.max(axis=0)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test = data_preprocessing.preprocess_data2(x_train_raw[:50000,:], y_train_raw[:50000], x_test_raw[:50000,:], data_annoted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8852ad8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8658, 745)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6f61bf54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8658, 745)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbaed223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'x_train_filtered = data_preprocessing.remove_useless(x_train_raw, data_annoted)\\nx_test_filtered = data_preprocessing.remove_useless(x_test_raw, data_annoted)'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"x_train_filtered = data_preprocessing.remove_useless(x_train_raw, data_annoted)\n",
    "x_test_filtered = data_preprocessing.remove_useless(x_test_raw, data_annoted)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf84a8bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data_train = data_preprocessing.clean_data(x_train_filtered[:100000,:], data_annoted)\\ndata_test = data_preprocessing.clean_data(x_test_filtered[:100000,:], data_annoted)'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"data_train = data_preprocessing.clean_data(x_train_filtered[:100000,:], data_annoted)\n",
    "data_test = data_preprocessing.clean_data(x_test_filtered[:100000,:], data_annoted)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "14361aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('x_train_preprocessed.csv', x_train, delimiter=',')\n",
    "np.savetxt('y_train_preprocessed.csv', y_train, delimiter=',')\n",
    "np.savetxt('x_test_preprocessed.csv', x_test, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ec040d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.loadtxt('x_train_preprocessed.csv', delimiter=',')\n",
    "x_test = np.loadtxt('x_test_preprocessed.csv', delimiter=',')\n",
    "y_train = np.loadtxt('y_train_preprocessed.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cfff5930",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christophersoriano/Library/Mobile Documents/com~apple~CloudDocs/Mac/Epfl/Master/MA3/Machine Learning/ML_project1/project1/implemented_functions.py:140: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  w = np.linalg.lstsq(tx,y)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least Squares: Loss = 0.07517940205799566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christophersoriano/Library/Mobile Documents/com~apple~CloudDocs/Mac/Epfl/Master/MA3/Machine Learning/ML_project1/project1/implemented_functions.py:53: RuntimeWarning: overflow encountered in matmul\n",
      "  MSE_loss = 1 / (2*N) * (e.T @ e)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularized Lasso Logistic Regression (gamma=0.01, lambda=0.1): Loss = 0.7295884189428956\n",
      "Regularized Lasso Logistic Regression (gamma=0.01, lambda=0.01): Loss = 0.6044027046131031\n",
      "Regularized Lasso Logistic Regression (gamma=0.1, lambda=0.1): Loss = 1.3572890022599247\n",
      "Regularized Lasso Logistic Regression (gamma=0.1, lambda=0.01): Loss = 0.5605006767270024\n",
      "Regularized Lasso Logistic Regression (gamma=0.5, lambda=0.1): Loss = 7.671478933726857\n",
      "Regularized Lasso Logistic Regression (gamma=0.5, lambda=0.01): Loss = 1.7106075762783193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christophersoriano/Library/Mobile Documents/com~apple~CloudDocs/Mac/Epfl/Master/MA3/Machine Learning/ML_project1/project1/implemented_functions.py:153: RuntimeWarning: invalid value encountered in matmul\n",
      "  A = tx.T @ tx + (N * lambda_) * np.eye(D)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression (alpha=0.001): Loss = 0.07260493581977448\n",
      "Ridge Regression (alpha=0.01): Loss = 0.07516525886456983\n",
      "Ridge Regression (alpha=0.1): Loss = 0.08009377609053892\n",
      "Ridge Regression (alpha=1): Loss = 0.09430405578945307\n",
      "Ridge Regression (alpha=10): Loss = 0.13107187002110993\n",
      "Logistic Regression: Loss = 0.555734407882561\n",
      "Logistic Regression: Loss = 0.48024830244186756\n",
      "Logistic Regression: Loss = 1.1433953522443614\n",
      "Logistic Regression: Loss = 2.1917776466757197\n",
      "Regularized Logistic Regression (alpha=0.001, gamma=0.01): Loss = 0.5558362953122725\n",
      "Regularized Logistic Regression (alpha=0.01, gamma=0.01): Loss = 0.5567479270599293\n",
      "Regularized Logistic Regression (alpha=0.1, gamma=0.01): Loss = 0.5653583478462917\n",
      "Regularized Logistic Regression (alpha=1, gamma=0.01): Loss = 0.6179510751211758\n",
      "Regularized Logistic Regression (alpha=0.001, gamma=0.1): Loss = 0.48084362732512487\n",
      "Regularized Logistic Regression (alpha=0.01, gamma=0.1): Loss = 0.48597637074064026\n",
      "Regularized Logistic Regression (alpha=0.1, gamma=0.1): Loss = 0.5222428120129543\n",
      "Regularized Logistic Regression (alpha=1, gamma=0.1): Loss = 0.6157002532087367\n",
      "Regularized Logistic Regression (alpha=0.001, gamma=0.5): Loss = 1.1577317484211744\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[72]\u001b[39m\u001b[32m, line 58\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m gamma \u001b[38;5;129;01min\u001b[39;00m [\u001b[32m0.01\u001b[39m, \u001b[32m0.1\u001b[39m, \u001b[32m0.5\u001b[39m]:\n\u001b[32m     57\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m reg_coef \u001b[38;5;129;01min\u001b[39;00m [\u001b[32m0.001\u001b[39m, \u001b[32m0.01\u001b[39m, \u001b[32m0.1\u001b[39m, \u001b[32m1\u001b[39m]:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m         w_reg_logistic, loss_reg_logistic = implemented_functions.reg_logistic_regression(y_train, x_train, reg_coef, initial_w, max_iters, gamma)\n\u001b[32m     59\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m loss_reg_logistic < best_log_reg_loss:\n\u001b[32m     60\u001b[39m             best_log_reg_loss = loss_reg_logistic\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Mobile Documents/com~apple~CloudDocs/Mac/Epfl/Master/MA3/Machine Learning/ML_project1/project1/implemented_functions.py:200\u001b[39m, in \u001b[36mreg_logistic_regression\u001b[39m\u001b[34m(y, tx, lambda_, initial_w, max_iters, gamma)\u001b[39m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iters):\n\u001b[32m    199\u001b[39m     gradient = compute_reg_logistic_gradient(y, tx, w, lambda_)\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m     w = w - gamma * gradient\n\u001b[32m    201\u001b[39m loss = compute_reg_logistic_loss(y, tx, w, lambda_)\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m w, loss\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import implemented_functions\n",
    "import importlib\n",
    "importlib.reload(implemented_functions)\n",
    "\n",
    "initial_w = np.zeros(x_train.shape[1])\n",
    "max_iters = 100\n",
    "gamma = 1\n",
    "\n",
    "x_test = x_train[7000:,:]\n",
    "y_test = y_train[7000:]\n",
    "x_train = x_train[:7000,:]\n",
    "y_train = y_train[:7000]\n",
    "\n",
    "\n",
    "w_least_squares, loss_ls = implemented_functions.least_squares(y_train, x_train)\n",
    "print(f\"Least Squares: Loss = {loss_ls}\")\n",
    "w_sgd, loss_sgd = implemented_functions.mean_squared_error_sgd(y_train,  x_train, initial_w, max_iters, gamma)\n",
    "\n",
    "\n",
    "best_log_lasso_coef = 0\n",
    "best_log_lasso_loss = np.inf\n",
    "best_log_lasso_weights = None\n",
    "for gamma in [0.01, 0.1, 0.5]:\n",
    "    for lambda_ in [0.1, 0.01]:\n",
    "        w_log_lasso, loss_log_lasso = implemented_functions.reg_logistic_lasso_subgradient(y_train, x_train, lambda_, initial_w, max_iters, gamma)\n",
    "        if loss_log_lasso < best_log_lasso_loss:\n",
    "            best_log_lasso_loss = loss_log_lasso\n",
    "            best_log_lasso_weights = w_log_lasso\n",
    "        print(f\"Regularized Lasso Logistic Regression (gamma={gamma}, lambda={lambda_}): Loss = {loss_log_lasso}\")\n",
    "        \n",
    "best_ridge_coef = 0\n",
    "best_ridge_loss = np.inf\n",
    "best_ridge_weights = None\n",
    "for ridge_coef in [0.001, 0.01, 0.1, 1, 10]:\n",
    "    w_ridge, loss_ridge = implemented_functions.ridge_regression(y_train, x_train, ridge_coef)\n",
    "    if loss_ridge < best_ridge_loss:\n",
    "        best_ridge_loss = loss_ridge\n",
    "        best_ridge_coef = ridge_coef\n",
    "        best_ridge_weights = w_ridge\n",
    "    print(f\"Ridge Regression (alpha={ridge_coef}): Loss = {loss_ridge}\")\n",
    "\n",
    "best_log_gamma = 0\n",
    "best_log_loss = np.inf\n",
    "best_log_weights = None\n",
    "for gamma in [0.01, 0.1, 0.5, 1]:\n",
    "    w_logistic, loss_logistic = implemented_functions.logistic_regression(y_train, x_train, initial_w, max_iters, gamma)\n",
    "    if loss_logistic < best_log_loss:\n",
    "        best_log_loss = loss_logistic\n",
    "        best_log_gamma = gamma\n",
    "        best_log_weights = w_logistic\n",
    "    print(f\"Logistic Regression: Loss = {loss_logistic}\")\n",
    "\n",
    "best_log_reg_coef = 0\n",
    "best_log_reg_loss = np.inf\n",
    "best_log_reg_weights = None\n",
    "for gamma in [0.01, 0.1, 0.5]:\n",
    "    for reg_coef in [0.001, 0.01, 0.1, 1]:\n",
    "        w_reg_logistic, loss_reg_logistic = implemented_functions.reg_logistic_regression(y_train, x_train, reg_coef, initial_w, max_iters, gamma)\n",
    "        if loss_reg_logistic < best_log_reg_loss:\n",
    "            best_log_reg_loss = loss_reg_logistic\n",
    "            best_log_reg_coef = reg_coef\n",
    "            best_log_reg_weights = w_reg_logistic\n",
    "        print(f\"Regularized Logistic Regression (alpha={reg_coef}, gamma={gamma}): Loss = {loss_reg_logistic}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e5dd05f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SGD is 49.819059107358264%\n",
      "Accuracy of Least Squares is 76.7189384800965%\n",
      "Accuracy of Ridge is 78.4680337756333%\n",
      "Accuracy of Logistic is 74.06513872135102%\n",
      "Accuracy of Regularized Logistic is 74.06513872135102%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 1.])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_accuracy(y_test, x_test, w, method):\n",
    "    y_pred = x_test@w\n",
    "    y_pred[y_pred <= 0.5] = 0\n",
    "    y_pred[y_pred > 0.5] = 1\n",
    "    computed_accuracy = np.sum(y_pred == y_test)/len(y_test)\n",
    "    print(f\"Accuracy of {method} is {computed_accuracy*100}%\")\n",
    "    return computed_accuracy, y_pred\n",
    "\n",
    "def test_methods(x_test, y_test, w_sgd, w_least_squares, best_ridge_weights, best_log_weights, best_log_reg_weights):\n",
    "    accuracy, y_pred = compute_accuracy(y_test, x_test, w_sgd, \"SGD\")\n",
    "    accuracy, y_pred = compute_accuracy(y_test, x_test, w_least_squares, \"Least Squares\")\n",
    "    accuracy, y_pred = compute_accuracy(y_test, x_test, best_ridge_weights, \"Ridge\")\n",
    "    accuracy, y_pred = compute_accuracy(y_test, x_test, best_log_weights, \"Logistic\")\n",
    "    accuracy, y_pred = compute_accuracy(y_test, x_test, best_log_reg_weights, \"Regularized Logistic\")\n",
    "    return y_pred\n",
    "\n",
    "test_methods(x_test, y_test, w_sgd, w_least_squares, best_ridge_weights, best_log_weights, best_log_reg_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1c63c635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "89041a9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m X_max = data_preprocessing.build_poly(x_train, \u001b[32m3\u001b[39m)\n\u001b[32m      2\u001b[39m initial_w = np.zeros(X_max.shape[\u001b[32m1\u001b[39m])\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m cross_validation.cross_validation_demo(y=y_train, x=x_train, k_fold=\u001b[32m5\u001b[39m, lambdas=[\u001b[32m0.001\u001b[39m], degree=\u001b[32m3\u001b[39m, initial_w=initial_w, max_iters=\u001b[32m100\u001b[39m, gamma=\u001b[32m0.1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Mobile Documents/com~apple~CloudDocs/Mac/Epfl/Master/MA3/Machine Learning/ML_project1/project1/cross_validation.py:87\u001b[39m, in \u001b[36mcross_validation_demo\u001b[39m\u001b[34m(y, x, degree, k_fold, lambdas, initial_w, max_iters, gamma, seed)\u001b[39m\n\u001b[32m     84\u001b[39m sums_te = {m: \u001b[32m0.0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m methods}\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(k_fold):\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     res = cross_validation(y, x, k_indices, k, lambda_, degree, initial_w, max_iters, gamma)\n\u001b[32m     88\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m methods:\n\u001b[32m     89\u001b[39m         tr_k, te_k = res[m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Mobile Documents/com~apple~CloudDocs/Mac/Epfl/Master/MA3/Machine Learning/ML_project1/project1/cross_validation.py:46\u001b[39m, in \u001b[36mcross_validation\u001b[39m\u001b[34m(y, x, k_indices, k, lambda_, degree, initial_w, max_iters, gamma)\u001b[39m\n\u001b[32m     44\u001b[39m w_mse_sgd, _loss_mse_sgd = mean_squared_error_sgd(y_tr_k, x_tr_k_poly, initial_w, max_iters, gamma)\n\u001b[32m     45\u001b[39m w_rdg, _loss_rdg         = ridge_regression(y_tr_k,       x_tr_k_poly, lambda_)\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m w_ls, _loss_ls           = least_squares(y_tr_k,          x_tr_k_poly)\n\u001b[32m     47\u001b[39m w_lr, _loss_lr           = logistic_regression(y_tr_k,    x_tr_k_poly, initial_w, max_iters, gamma)\n\u001b[32m     48\u001b[39m w_reg_lr, _loss_reg_lr   = reg_logistic_regression(y_tr_k,x_tr_k_poly, lambda_, initial_w, max_iters, gamma)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Mobile Documents/com~apple~CloudDocs/Mac/Epfl/Master/MA3/Machine Learning/ML_project1/project1/implemented_functions.py:140\u001b[39m, in \u001b[36mleast_squares\u001b[39m\u001b[34m(y, tx)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mleast_squares\u001b[39m(y,tx) : \n\u001b[32m    129\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    Performs the ordinary least squares regression algorithm and returns the best parameters, along with its associated loss\u001b[39;00m\n\u001b[32m    131\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    138\u001b[39m \u001b[33;03m        loss : scalar, mean squared error loss\u001b[39;00m\n\u001b[32m    139\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m     w = np.linalg.lstsq(tx,y)[\u001b[32m0\u001b[39m]\n\u001b[32m    141\u001b[39m     loss = compute_loss(y,tx,w)\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m w,loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/miniconda3/envs/MainPy/lib/python3.11/site-packages/numpy/linalg/linalg.py:2326\u001b[39m, in \u001b[36mlstsq\u001b[39m\u001b[34m(a, b, rcond)\u001b[39m\n\u001b[32m   2323\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_rhs == \u001b[32m0\u001b[39m:\n\u001b[32m   2324\u001b[39m     \u001b[38;5;66;03m# lapack can't handle n_rhs = 0 - so allocate the array one larger in that axis\u001b[39;00m\n\u001b[32m   2325\u001b[39m     b = zeros(b.shape[:-\u001b[32m2\u001b[39m] + (m, n_rhs + \u001b[32m1\u001b[39m), dtype=b.dtype)\n\u001b[32m-> \u001b[39m\u001b[32m2326\u001b[39m x, resids, rank, s = gufunc(a, b, rcond, signature=signature, extobj=extobj)\n\u001b[32m   2327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m m == \u001b[32m0\u001b[39m:\n\u001b[32m   2328\u001b[39m     x[...] = \u001b[32m0\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "X_max = data_preprocessing.build_poly(x_train, 3)\n",
    "initial_w = np.zeros(X_max.shape[1])\n",
    "\n",
    "cross_validation.cross_validation_demo(y=y_train, x=x_train, k_fold=5, lambdas=[0.001], degree=3, initial_w=initial_w, max_iters=100, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41833652",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MainPy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
